# MIT License
#
# Copyright (c) 2023 Ivan Specht
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
#   The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

library(ape)

fasta <- read.FASTA("~/Desktop/100k/aligned.fasta")


names <- names(fasta)
names <- gsub("\\|.*", "", names)

vcf_names <- list.files("~/Desktop/100k/vcf/")
vcf_names <- gsub("\\..*", "", vcf_names)

filter_fasta <- fasta[names %in% vcf_names]

filter_names <- names(filter_fasta)
filter_dates <- sub(".*\\|", "", filter_names)
filter_names <- sub("\\|.*", "", filter_names)

sum(vcf_names %in% names)

names(filter_fasta) <- filter_names

write.FASTA(filter_fasta, "~/Desktop/100k/aligned.fasta")

dates <- as.Date(filter_dates)
dates <- as.numeric(difftime(dates, as.Date("2020-01-01"), units = "days"))

dates <- data.frame(cases = filter_names, date = dates)

write.csv(dates, file = "~/Desktop/100k/date.csv", quote = F, row.names = F)

### Develop initial ancestry based on lineage

meta <- read.csv("~/Desktop/input_data_huge/metadata.csv")
meta <- meta[match(filter_names, meta$case), ]
meta <- meta[, c("case", "pango_lineage_full")]
row.names(meta) <- NULL

# Delete annotations of lineages
meta$pango_lineage_full <- sub(" .*", "", meta$pango_lineage_full)

# Find ancestral lineage of each lineage. Hopefully we get a tree...
lins <- sort(unique(meta$pango_lineage_full))

split_lins <- sapply(lins, strsplit, split = "\\.", USE.NAMES = F)

anc_lins <- c()
for (i in 1:length(split_lins)) {
  older <- which(sapply(split_lins, function(s, t){all(s == t[1:length(s)])}, t = split_lins[[i]], USE.NAMES = F))
  older <- setdiff(older, i)
  if(length(older) > 0){
    anc_lins[i] <- lins[older][which.max(sapply(split_lins[older], length))]
  }else{
    anc_lins[i] <- NA
  }

}

## Next, find a root for each lineage
roots <- c()
for (i in 1:length(lins)) {
  cases <- meta$case[meta$pango_lineage_full == lins[i]]
  times <- dates$date[match(cases, dates$cases)]
  roots[i] <- cases[which.min(times)]
}

## Finally, make the ancestry vector!
h <- c()
for (i in 1:nrow(dates)) {
  # If not a root, assign ancestor to be root of cluster
  if(!(meta$case[i] %in% roots)){
    lin <- meta$pango_lineage_full[i]
    root <- roots[which(lins == lin)]
    h[i] <- which(meta$case == root)
  }else{
    lin <- anc_lins[which(roots == meta$case[i])]
    if(is.na(lin)){
      h[i] <- 0
    }else{
      root <- roots[which(lins == lin)]
      h[i] <- which(meta$case == root)
    }
  }
  #print(i)
}

h <- h + 1
h <- c(NA, h)

write.table(h, file = "~/Desktop/100k/ancestry.csv", quote = F, row.names = F, col.names = F)


### Rename vcfs
n_vcf <- length(list.files("./input_data_huge/vcf"))
old_names <- list.files("./input_data_huge/vcf")
new_names <- gsub("\\..*", "", old_names)
newer_names <- paste0(new_names, ".vcf")


file.rename(new_names, newer_names)


### Assemble trial dataset
meta$Collection.Date <- as.Date(meta$Collection.Date)
hist(meta$Collection.Date, breaks = "weeks")
min_date <- min(meta$Collection.Date)
unique(meta$Practice.Name[date <= meta$Collection.Date & meta$Collection.Date <= date + 10])

date <- as.Date("2022-06-01")
sum(date <= meta$Collection.Date & meta$Collection.Date <= date + 10)

date_range <- as.numeric(difftime(date, as.Date("2020-01-01"), units = "days"))

sort(table(meta$Practice.Name))



cons <- read.FASTA("~/Desktop/input_data_huge/aligned.fasta")
date <- read.csv("~/Desktop/input_data_huge/date.csv")
sub <- meta[meta$Program2 == "Prison", ]
keep <- which(names(cons) %in% sub$case)
cons <- cons[keep]
date <- date[keep,]

write.FASTA(cons, "~/Desktop/input_data/aligned.fasta")
write.csv(date, file = "~/Desktop/input_data/date.csv", quote = F, row.names = F)

cmds <- paste0("cp ~/Desktop/input_data_huge/vcf/", names(cons), ".vcf ./vcf")
cmds <- c(cmds, "cp ~/Desktop/input_data_huge/ref.fasta .")
writeLines(cmds, "~/Desktop/input_data/script.txt")


### Filter for common iSNVs
context <- read.csv("~/Desktop/reconstructR/inst/extdata/context.csv")
tot_isnvs <- data.frame(MUT = context$aa_change_full, POS = context$POS, COUNT = Rfast::rowsums(as.matrix(context[,4:ncol(context)]), na.rm = T))
N_isnv_cases <- sum(tot_isnvs$COUNT)
N_context <- 172519

hgeom_probs <- c()
for (i in 1:29903) {
  sub <- tot_isnvs[tot_isnvs$POS == i, ]
  hgeom_probs[i] <- 1 - phyper(1, sum(sub$COUNT), N_context - sum(sub$COUNT), 1000) # Probability
}


# What sites have a probability exceeding 0.05?
problem_sites <- which(hgeom_probs > 0.05)

write.csv(problem_sites, "~/Desktop/problematic.csv", quote = F, row.names = F)
