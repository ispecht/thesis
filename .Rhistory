combos[13:14, 7] <- vs
run_combo <- function(i, mcmc){
mcmc$a_g <- combos[i, 1]
mcmc$lambda_g <- combos[1, 2]
mcmc$a_s <- combos[i, 3]
mcmc$lambda_s <- combos[i, 4]
mcmc$rho <- combos[i, 5]
mcmc$psi <- combos[i, 6]
mcmc$v <- combos[i, 7]
out <- run_mcmc(mcmc, data)
return(out)
}
combos
outs <- mclapply(1:nrow(combos), run_combo, mcmc = mcmc, mc.cores = 1)
outs <- mclapply(1:nrow(combos), run_combo, mcmc = mcmc, mc.cores = 2)
#save(outs, file = "outs.RData")
load("outs.RData")
sensitivity <- list()
all_liks <- list()
for (i in 1:nrow(combos)) {
all_liks[[i]] <- outs[[i]][[1]]
sensitivity[[i]] <- outs[[i]][[2]]
}
## Get adjacency matrix. Ancestor > data$n_obs doesn't count for anything. Indirect transmission OK
get_adj <- function(run, n_obs){
out <- matrix(0, nrow = n_obs, ncol = n_obs)
len <- length(run)
for (i in (len*0.1):len) {
h <- run[[i]]$h
present <- which(!is.na(h) & h <= n_obs)
present <- present[present <= n_obs]
out[cbind(h[present], present)] <- out[cbind(h[present], present)] + 1
}
return(out / (0.9*len))
}
adjs <- lapply(sensitivity, get_adj, n_obs = data$n_obs)
# Run on default settings
default <- outs[[nrow(combos)]]
default_adj <- get_adj(default[[2]], data$n_obs)
# Histogram of change in adjacency matrix
network_change <- function(i){
change <- as.vector(adjs[[i]] - default_adj)
change <- change[abs(change) > 0]
p <- ggplot(data.frame(x = change), aes(x = x)) +
geom_histogram(aes(y=after_stat(density)), binwidth = 0.1, boundary = 0.1, color = "white", fill = "grey") +
xlab("Change in Posterior Probability") +
ylab("Probability Density") +
#scale_y_continuous(trans='log2') +
xlim(-1, 1) +
theme_minimal()
p
}
hists <- lapply(1:nrow(combos), network_change)
all_hists <- plot_grid(
hists[[1]],
hists[[2]],
hists[[3]],
hists[[4]],
hists[[5]],
hists[[6]],
hists[[7]],
hists[[8]],
hists[[9]],
hists[[10]],
hists[[11]],
hists[[12]],
hists[[13]],
hists[[14]],
hists[[15]],
ncol = 3,
labels = "AUTO"
)
#library(ggraph)
library(ggplot2)
hists <- lapply(1:nrow(combos), network_change)
all_hists <- plot_grid(
hists[[1]],
hists[[2]],
hists[[3]],
hists[[4]],
hists[[5]],
hists[[6]],
hists[[7]],
hists[[8]],
hists[[9]],
hists[[10]],
hists[[11]],
hists[[12]],
hists[[13]],
hists[[14]],
hists[[15]],
ncol = 3,
labels = "AUTO"
)
### Execute large-scale outbreak reconstruction algorithm
set.seed(232)
## Libraries
library(ape)
library(Rcpp)
library(igraph)
#library(ggraph)
library(ggplot2)
library(cowplot)
library(parallel)
all_hists <- plot_grid(
hists[[1]],
hists[[2]],
hists[[3]],
hists[[4]],
hists[[5]],
hists[[6]],
hists[[7]],
hists[[8]],
hists[[9]],
hists[[10]],
hists[[11]],
hists[[12]],
hists[[13]],
hists[[14]],
hists[[15]],
ncol = 3,
labels = "AUTO"
)
ggsave("./figs/hist.pdf", width = 7.8, height = 9)
ggsave("./figs/hist.png", width = 7.8, height = 9)
## Next up, density plots of mu
param_dens <- function(i, param){
run <- sensitivity[[i]]
out <- c()
len <- length(run)
for (j in (len*0.1):len) {
out <- c(out, (run[[j]][[param]]))
}
p <- ggplot(data.frame(x = out), aes(x = x)) +
geom_histogram(aes(y=after_stat(density)), color = "white", fill = "grey") +
xlab(paste("Value of", param)) +
ylab("Probability Density") +
scale_x_continuous(breaks = signif(seq(min(out), max(out), by = (max(out) - min(out)) / 2) , 2)) +
scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +
theme_minimal() +
theme(plot.margin = margin(t = 0, r = 0.2, b = 0, l = 0, unit = "in"))
p
}
output_plots <- function(param){
dens <- lapply(1:nrow(combos), param_dens, param = param)
all_dens <- plot_grid(
dens[[1]],
dens[[2]],
dens[[3]],
dens[[4]],
dens[[5]],
dens[[6]],
dens[[7]],
dens[[8]],
dens[[9]],
dens[[10]],
dens[[11]],
dens[[12]],
dens[[13]],
dens[[14]],
dens[[15]],
ncol = 3,
labels = "AUTO"
)
ggsave(paste0("./figs/", param, ".pdf"), width = 7.8, height = 10.2)
ggsave(paste0("./figs/", param, ".png"), width = 7.8, height = 10.2)
}
output_plots("mu")
output_plots("p")
#output_plots("lambda")
output_plots("b")
combos
### Sensitivity analysis
# Parameters to assess sensitivity: (as changes from default values)
source("main.R")
## Unhash these for regular run
init <- initialize(init_mst = T)
#init <- initialize()
mcmc <- init[[1]]
data <- init[[2]]
data$n_local = 10
data$sample_every = 10
data$n_global = 100000
## Unhash these for run on pre-computed data and initial MCMC
# load("data.RData")
# load("mcmc.RData")
a_gs <- c(4, 6) # Default = 5
lambda_gs <- a_gs/5 # Default = 1. Also update a_g to maintain mean of 5. Corresponds to variance of 5 (default), 25, 1
a_ss <- c(4, 6) # Default = 5
lambda_ss <- a_ss/5 # Default = 1. Also update a_s to maintain mean of 5. Corresponds to variance of 5 (default), 25, 1
rhos <- c(0.05, 0.2) # Default = 0.1. Also update psi = rho / (2.5 + rho)
psis <- 0.1 / (c(1.5, 3.5) + 0.1)
vs <- c(500, 2000)
# jth column is corresponds to each of the above params, in order
combos <- matrix(c(5, 1, 5, 1, 0.1, 0.1/(2.5 + 0.1), 1000), nrow = 15, ncol = 7, byrow = T)
combos[1:2, 1] <- a_gs
combos[3:4, 1] <- lambda_gs * 5
combos[3:4, 2] <- lambda_gs
combos[5:6, 3] <- a_ss
combos[7:8, 3] <- lambda_ss * 5
combos[7:8, 4] <- lambda_ss
combos[9:10, 5] <- rhos
combos[9:10, 6] <- rhos / (2.5 + rhos)
combos[11:12, 6] <- psis
combos[13:14, 7] <- vs
run_combo <- function(i, mcmc){
mcmc$a_g <- combos[i, 1]
mcmc$lambda_g <- combos[1, 2]
mcmc$a_s <- combos[i, 3]
mcmc$lambda_s <- combos[i, 4]
mcmc$rho <- combos[i, 5]
mcmc$psi <- combos[i, 6]
mcmc$v <- combos[i, 7]
out <- run_mcmc(mcmc, data)
return(out)
}
outs <- mclapply(1:nrow(combos), run_combo, mcmc = mcmc, mc.cores = 1)
### Sensitivity analysis
# Parameters to assess sensitivity: (as changes from default values)
source("main.R")
## Unhash these for regular run
init <- initialize(init_mst = T)
#init <- initialize()
mcmc <- init[[1]]
data <- init[[2]]
data$n_local = 10
data$sample_every = 10
data$n_global = 100000
## Unhash these for run on pre-computed data and initial MCMC
# load("data.RData")
# load("mcmc.RData")
a_gs <- c(4, 6) # Default = 5
lambda_gs <- a_gs/5 # Default = 1. Also update a_g to maintain mean of 5. Corresponds to variance of 5 (default), 25, 1
a_ss <- c(4, 6) # Default = 5
lambda_ss <- a_ss/5 # Default = 1. Also update a_s to maintain mean of 5. Corresponds to variance of 5 (default), 25, 1
rhos <- c(0.05, 0.2) # Default = 0.1. Also update psi = rho / (2.5 + rho)
psis <- 0.1 / (c(1.5, 3.5) + 0.1)
vs <- c(500, 2000)
# jth column is corresponds to each of the above params, in order
combos <- matrix(c(5, 1, 5, 1, 0.1, 0.1/(2.5 + 0.1), 1000), nrow = 15, ncol = 7, byrow = T)
combos[1:2, 1] <- a_gs
combos[3:4, 1] <- lambda_gs * 5
combos[3:4, 2] <- lambda_gs
combos[5:6, 3] <- a_ss
combos[7:8, 3] <- lambda_ss * 5
combos[7:8, 4] <- lambda_ss
combos[9:10, 5] <- rhos
combos[9:10, 6] <- rhos / (2.5 + rhos)
combos[11:12, 6] <- psis
combos[13:14, 7] <- vs
run_combo <- function(i, mcmc){
mcmc$a_g <- combos[i, 1]
mcmc$lambda_g <- combos[1, 2]
mcmc$a_s <- combos[i, 3]
mcmc$lambda_s <- combos[i, 4]
mcmc$rho <- combos[i, 5]
mcmc$psi <- combos[i, 6]
mcmc$v <- combos[i, 7]
out <- run_mcmc(mcmc, data)
return(out)
}
### Sensitivity analysis
# Parameters to assess sensitivity: (as changes from default values)
source("main.R")
## Unhash these for regular run
init <- initialize(init_mst = T)
#init <- initialize()
mcmc <- init[[1]]
data <- init[[2]]
data$n_local = 10
data$sample_every = 10
data$n_global = 100000
## Unhash these for run on pre-computed data and initial MCMC
# load("data.RData")
# load("mcmc.RData")
a_gs <- c(4, 6) # Default = 5
lambda_gs <- a_gs/5 # Default = 1. Also update a_g to maintain mean of 5. Corresponds to variance of 5 (default), 25, 1
a_ss <- c(4, 6) # Default = 5
lambda_ss <- a_ss/5 # Default = 1. Also update a_s to maintain mean of 5. Corresponds to variance of 5 (default), 25, 1
rhos <- c(0.05, 0.2) # Default = 0.1. Also update psi = rho / (2.5 + rho)
psis <- 0.1 / (c(1.5, 3.5) + 0.1)
vs <- c(500, 2000)
# jth column is corresponds to each of the above params, in order
combos <- matrix(c(5, 1, 5, 1, 0.1, 0.1/(2.5 + 0.1), 1000), nrow = 15, ncol = 7, byrow = T)
combos[1:2, 1] <- a_gs
combos[3:4, 1] <- lambda_gs * 5
combos[3:4, 2] <- lambda_gs
combos[5:6, 3] <- a_ss
combos[7:8, 3] <- lambda_ss * 5
combos[7:8, 4] <- lambda_ss
combos[9:10, 5] <- rhos
combos[9:10, 6] <- rhos / (2.5 + rhos)
combos[11:12, 6] <- psis
combos[13:14, 7] <- vs
run_combo <- function(i, mcmc){
mcmc$a_g <- combos[i, 1]
mcmc$lambda_g <- combos[1, 2]
mcmc$a_s <- combos[i, 3]
mcmc$lambda_s <- combos[i, 4]
mcmc$rho <- combos[i, 5]
mcmc$psi <- combos[i, 6]
mcmc$v <- combos[i, 7]
out <- run_mcmc(mcmc, data)
return(out)
}
outs <- mclapply(1:nrow(combos), run_combo, mcmc = mcmc, mc.cores = 1)
getwd()
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
### Simulate outbreak
set.seed(211)
library(stringr)
library(ape)
setwd("~/Desktop/thesis")
source("main.R")
### Generate transmission network
# Epi params
a_g <- 5
lambda_g <- 1
a_s <- 5
lambda_s <- 1
mu <- 2e-6
p <- 1e-6
v <- 1000 # virions produced per cycle
b <- 0.05 # probability bottleneck has size 2
k <- round(1/sqrt(p)) # number of virions at end of exponential growth phase
N_bases <- 29903 # length of viral genome
rho <- 0.5
psi <- 0.5 / (1.5 + 0.5)
N <- 1e9
# Probability of a mutation in exponential growth phase
p_growth_mut <- 1 - (1-p)^k
# Number of generations to simulate
G <- 4
setwd("./newdir_1/")
init <- initialize()
#init <- initialize()
mcmc <- init[[1]]
data <- init[[2]]
data$n_local = 10
data$sample_every = 10
data$n_global = 1000
mcmc$rho <- 0.5
mcmc$psi <- 0.5 / (1.5 + 0.5)
coalescent(mcmc, data)
gens <- gen(mcmc$h, mcmc$w)
G <- max(gens)
qs <- get_qs(mcmc$n - 1 + sum(mcmc$w), data$N, G, mcmc$rho, mcmc$psi)
qs
exp(qs)
sum(mcmc$d * log((1-mcmc$psi) / mcmc$psi) + lchoose(mcmc$d + mcmc$rho - 1, mcmc$d) - lchoose(data$N, mcmc$d)) +
sum(mcmc$w) * (log(mcmc$rho) + log((1-mcmc$psi) / mcmc$psi) - log(data$N))
lchoose(data$N, mcmc$n - data$n_obs) + lfactorial(mcmc$n - data$n_obs) - (mcmc$n - 1)*log(data$N)
lchoose(data$N, mcmc$n - data$n_obs) + lfactorial(mcmc$n - data$n_obs)
sum(mcmc$w)
# Recursive form of epi likelihood
coalescent <- function(mcmc, data){
gens <- gen(mcmc$h, mcmc$w)
G <- max(gens)
qs <- get_qs(mcmc$n - 1 + sum(mcmc$w), data$N, G, mcmc$rho, mcmc$psi)
out <- 0
for (i in 1:mcmc$n) {
out <- out + e_lik_node(mcmc$d[i], mcmc$w[i], qs[(gens[i] + 1 - mcmc$w[i]):(gens[i] + 1)], mcmc$rho, mcmc$psi)
}
out <- out + lchoose(data$N, mcmc$n - data$n_obs + sum(mcmc$w)) + lfactorial(mcmc$n - data$n_obs + sum(mcmc$w)) - (mcmc$n - 1 + sum(mcmc$w))*log(data$N)
return(out)
}
coalescent(mcmc, data)
lchoose(data$N, mcmc$n - data$n_obs + sum(mcmc$w)) + lfactorial(mcmc$n - data$n_obs + sum(mcmc$w))
sum(mcmc$d * log((1-mcmc$psi) / mcmc$psi) + lchoose(mcmc$d + mcmc$rho - 1, mcmc$d) - lchoose(data$N, mcmc$d)) +
sum(mcmc$w) * (log(mcmc$rho) + log((1-mcmc$psi) / mcmc$psi) - log(data$N)) +
lchoose(data$N, mcmc$n - data$n_obs + sum(mcmc$w)) + lfactorial(mcmc$n - data$n_obs + sum(mcmc$w))
return(
# Generation intervals
sum(dgamma(mcmc$t[2:mcmc$n] - mcmc$t[mcmc$h[2:mcmc$n]], shape = (mcmc$w[2:mcmc$n] + 1) * mcmc$a_g, rate = mcmc$lambda_g, log = T)) +
# Sojourn intervals
sum(dgamma(data$s[2:data$n_obs] - mcmc$t[2:data$n_obs], shape = mcmc$a_s, rate = mcmc$lambda_s, log = T)) +
# Varilly Coalescent
sum(mcmc$d * log((1-mcmc$psi) / mcmc$psi) + lchoose(mcmc$d + mcmc$rho - 1, mcmc$d) - lchoose(data$N, mcmc$d)) +
sum(mcmc$w) * (log(mcmc$rho) + log((1-mcmc$psi) / mcmc$psi) - log(data$N)) +
lchoose(data$N, mcmc$n - data$n_obs + sum(mcmc$w)) + lfactorial(mcmc$n - data$n_obs + sum(mcmc$w))
coalescent(mcmc, data)
coalescent(mcmc, data)
e_lik
getwd()
set.seed(211)
library(stringr)
library(ape)
setwd("~/Desktop/thesis")
source("main.R")
e_lik
setwd("./newdir_1/")
init <- initialize()
#init <- initialize()
mcmc <- init[[1]]
data <- init[[2]]
data$n_local = 10
data$sample_every = 10
data$n_global = 1000
mcmc$rho <- 0.5
mcmc$psi <- 0.5 / (1.5 + 0.5)
output <- run_mcmc(mcmc, data)
library(ggraph)
### M-H algo
run_mcmc <- function(mcmc, data, noisy = F){
output <- list()
liks <- c()
for (r in 1:data$n_global) {
# For reproducible results
#set.seed(r)
# Make global moves
mcmc <- global_mcmc(mcmc, data)
# Chop up the tree into pieces
breakdowns <- breakdown(mcmc, data)
mcmcs <- breakdowns[[1]]
datas <- breakdowns[[2]]
if(noisy){
message(paste("Parallelizing over", length(mcmcs), "cores..."))
}
# all_res <- parallel::mclapply(
#   1:length(mcmcs),
#   function(i, mcmcs, datas){
#     local_mcmc(mcmcs[[i]], datas[[i]])
#   },
#   mcmcs = mcmcs,
#   datas = datas,
#   mc.set.seed = F,
#   mc.cores = length(mcmcs)
# )
#...or run in series
all_res <- list()
for (j in 1:length(mcmcs)) {
all_res[[j]] <- local_mcmc(mcmcs[[j]], datas[[j]])
}
# Amalgamate results of parallel MCMC run
amalgam <- amalgamate(all_res, mcmcs, datas, mcmc, data)
# Record amalgamated results, filtering to parameters of interest
for (i in 1:length(amalgam)) {
output <- c(output, list(
amalgam[[i]][data$record]
))
}
# "mcmc" is now the most recent result
mcmc <- amalgam[[length(amalgam)]]
#print(r)
liks <- c(liks, mcmc$e_lik + sum(mcmc$g_lik[2:mcmc$n]) + mcmc$prior)
if(noisy){
message(paste(r, "global iterations complete. Log-likelihood =", round(liks[r], 2)))
print(plot_current(mcmc$h, data$n_obs))
#print(mcmc$w)
print(mcmc$mu)
print(mcmc$p)
# print(length(unlist(mcmc$m01)) + length(unlist(mcmc$m10)))
# print(length(unlist(mcmc$mx1)))
#print(data$s - mcmc$t[1:data$n_obs])
#print(mcmc$lambda)
#print(mcmc$h)
# print(mcmc$a_g)
}
# if(r == 10){
#   data$n_subtrees <- 3
# }
}
return(list(
liks, output
))
}
output <- run_mcmc(mcmc, data, noisy = T)
e_lik <- function(mcmc, data){
if(
any(mcmc$w < 0) |
mcmc$a_g < 0 |
mcmc$lambda_g < 0 |
mcmc$a_s < 0 |
mcmc$lambda_s < 0 |
mcmc$rho < 0 |
mcmc$psi < 0
){
return(-Inf)
}else{
return(
# Generation intervals
sum(dgamma(mcmc$t[2:mcmc$n] - mcmc$t[mcmc$h[2:mcmc$n]], shape = (mcmc$w[2:mcmc$n] + 1) * mcmc$a_g, rate = mcmc$lambda_g, log = T)) +
# Sojourn intervals
sum(dgamma(data$s[2:data$n_obs] - mcmc$t[2:data$n_obs], shape = mcmc$a_s, rate = mcmc$lambda_s, log = T)) +
# Varilly Coalescent
# sum(mcmc$d * log((1-mcmc$psi) / mcmc$psi) + lchoose(mcmc$d + mcmc$rho - 1, mcmc$d) - lchoose(data$N, mcmc$d)) +
# sum(mcmc$w) * (log(mcmc$rho) + log((1-mcmc$psi) / mcmc$psi) - log(data$N)) +
# lchoose(data$N, mcmc$n - data$n_obs + sum(mcmc$w)) + lfactorial(mcmc$n - data$n_obs + sum(mcmc$w))
coalescent(mcmc, data)
)
}
}
e_lik
init <- initialize()
#init <- initialize()
mcmc <- init[[1]]
data <- init[[2]]
data$n_local = 10
data$sample_every = 10
data$n_global = 1000
mcmc$rho <- 0.5
mcmc$psi <- 0.5 / (1.5 + 0.5)
output <- run_mcmc(mcmc, data, noisy = T)
h
